{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will collect different chunking algorithms that claim to provide _optimal_ access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unidata Algorithm\n",
    "\n",
    "Claims to provide optimized chunking of 3d variables for balanced 1d and 2d access.\n",
    "\n",
    "The code fetched from: http://www.unidata.ucar.edu/staff/russ/public/chunk_shape_3D.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binlist(n, width=0):\n",
    "    \"\"\"Return list of bits that represent a non-negative integer.\n",
    "\n",
    "    n      -- non-negative integer\n",
    "    width  -- number of bits in returned zero-filled list (default 0)\n",
    "    \"\"\"\n",
    "    return map(int, list(bin(n)[2:].zfill(width)))\n",
    "\n",
    "def numVals(shape):\n",
    "    \"\"\"Return number of values in chunk of specified shape, given by a list of dimension lengths.\n",
    "\n",
    "    shape -- list of variable dimension sizes\"\"\"\n",
    "    if(len(shape) == 0):\n",
    "        return 1\n",
    "    return np.prod(shape)\n",
    "\n",
    "def perturbShape(shape, onbits):\n",
    "    \"\"\"Return shape perturbed by adding 1 to elements corresponding to 1 bits in onbits\n",
    "\n",
    "    shape  -- list of variable dimension sizes\n",
    "    onbits -- non-negative integer less than 2**len(shape)\n",
    "    \"\"\"\n",
    "    return list(map(sum, zip(shape, binlist(onbits, len(shape)))))\n",
    "\n",
    "def chunk_shape_3D(varShape, valSize=4, chunkSize=4096):\n",
    "    \"\"\"\n",
    "    Return a 'good shape' for a 3D variable, assuming balanced 1D, 2D access\n",
    "\n",
    "    varShape  -- length 3 list of variable dimension sizes\n",
    "    chunkSize -- maximum chunksize desired, in bytes (default 4096)\n",
    "    valSize   -- size of each data value, in bytes (default 4)\n",
    "\n",
    "    Returns integer chunk lengths of a chunk shape that provides\n",
    "    balanced access of 1D subsets and 2D subsets of a netCDF or HDF5\n",
    "    variable var with shape (T, X, Y), where the 1D subsets are of the\n",
    "    form var[:,x,y] and the 2D slices are of the form var[t,:,:],\n",
    "    typically 1D time series and 2D spatial slices.  'Good shape' for\n",
    "    chunks means that the number of chunks accessed to read either\n",
    "    kind of 1D or 2D subset is approximately equal, and the size of\n",
    "    each chunk (uncompressed) is no more than chunkSize, which is\n",
    "    often a disk block size.\n",
    "    \"\"\"\n",
    "\n",
    "    rank = 3  # this is a special case of n-dimensional function chunk_shape\n",
    "    chunkVals = chunkSize / float(valSize) # ideal number of values in a chunk\n",
    "    numChunks  = varShape[0]*varShape[1]*varShape[2] / chunkVals # ideal number of chunks\n",
    "    axisChunks = numChunks ** 0.25 # ideal number of chunks along each 2D axis\n",
    "    cFloor = [] # will be first estimate of good chunk shape\n",
    "    # cFloor  = [varShape[0] // axisChunks**2, varShape[1] // axisChunks, varShape[2] // axisChunks]\n",
    "    # except that each chunk shape dimension must be at least 1\n",
    "    # chunkDim = max(1.0, varShape[0] // axisChunks**2)\n",
    "    if varShape[0] / axisChunks**2 < 1.0:\n",
    "        chunkDim = 1.0\n",
    "        axisChunks = axisChunks / math.sqrt(varShape[0]/axisChunks**2)\n",
    "    else:\n",
    "        chunkDim = varShape[0] // axisChunks**2\n",
    "    cFloor.append(chunkDim)\n",
    "    prod = 1.0  # factor to increase other dims if some must be increased to 1.0\n",
    "    for i in range(1, rank):\n",
    "        if varShape[i] / axisChunks < 1.0:\n",
    "            prod *= axisChunks / varShape[i]\n",
    "    for i in range(1, rank):\n",
    "        if varShape[i] / axisChunks < 1.0:\n",
    "            chunkDim = 1.0\n",
    "        else:\n",
    "            chunkDim = (prod*varShape[i]) // axisChunks\n",
    "        cFloor.append(chunkDim)\n",
    "\n",
    "    # cFloor is typically too small, (numVals(cFloor) < chunkSize)\n",
    "    # Adding 1 to each shape dim results in chunks that are too large,\n",
    "    # (numVals(cCeil) > chunkSize).  Want to just add 1 to some of the\n",
    "    # axes to get as close as possible to chunkSize without exceeding\n",
    "    # it.  Here we use brute force, compute numVals(cCand) for all\n",
    "    # 2**rank candidates and return the one closest to chunkSize\n",
    "    # without exceeding it.\n",
    "    bestChunkSize = 0\n",
    "    cBest = cFloor\n",
    "    for i in range(8):\n",
    "        # cCand = map(sum,zip(cFloor, binlist(i, rank)))\n",
    "        cCand = perturbShape(cFloor, i)\n",
    "        thisChunkSize = valSize * numVals(cCand)\n",
    "        if bestChunkSize < thisChunkSize <= chunkSize:\n",
    "            bestChunkSize = thisChunkSize\n",
    "            cBest = list(cCand) # make a copy of best candidate so far\n",
    "    return list(map(int, cBest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `h5py` Algorithm\n",
    "\n",
    "From: https://github.com/h5py/h5py/blob/master/h5py/_hl/filters.py#L252\n",
    "\n",
    "Used when dataset to be created requires chunking but the user has not provided chunk shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def guess_chunk(shape, maxshape, typesize):\n",
    "    \"\"\" Guess an appropriate chunk layout for a dataset, given its shape and\n",
    "    the size of each element in bytes.  Will allocate chunks only as large\n",
    "    as MAX_SIZE.  Chunks are generally close to some power-of-2 fraction of\n",
    "    each axis, slightly favoring bigger values for the last index.\n",
    "    Undocumented and subject to change without warning.\n",
    "    \"\"\"\n",
    "    CHUNK_BASE = 16*1024    # Multiplier by which chunks are adjusted\n",
    "    CHUNK_MIN = 8*1024      # Soft lower limit (8k)\n",
    "    CHUNK_MAX = 1024*1024   # Hard upper limit (1M)\n",
    "    \n",
    "    # For unlimited dimensions we have to guess 1024\n",
    "    shape = tuple((x if x!=0 else 1024) for i, x in enumerate(shape))\n",
    "\n",
    "    ndims = len(shape)\n",
    "    if ndims == 0:\n",
    "        raise ValueError(\"Chunks not allowed for scalar datasets.\")\n",
    "\n",
    "    chunks = np.array(shape, dtype='=f8')\n",
    "    if not np.all(np.isfinite(chunks)):\n",
    "        raise ValueError(\"Illegal value in chunk tuple\")\n",
    "\n",
    "    # Determine the optimal chunk size in bytes using a PyTables expression.\n",
    "    # This is kept as a float.\n",
    "    dset_size = np.product(chunks)*typesize\n",
    "    target_size = CHUNK_BASE * (2**np.log10(dset_size/(1024.*1024)))\n",
    "\n",
    "    if target_size > CHUNK_MAX:\n",
    "        target_size = CHUNK_MAX\n",
    "    elif target_size < CHUNK_MIN:\n",
    "        target_size = CHUNK_MIN\n",
    "\n",
    "    idx = 0\n",
    "    while True:\n",
    "        # Repeatedly loop over the axes, dividing them by 2.  Stop when:\n",
    "        # 1a. We're smaller than the target chunk size, OR\n",
    "        # 1b. We're within 50% of the target chunk size, AND\n",
    "        #  2. The chunk is smaller than the maximum chunk size\n",
    "\n",
    "        chunk_bytes = np.product(chunks)*typesize\n",
    "\n",
    "        if (chunk_bytes < target_size or \\\n",
    "         abs(chunk_bytes-target_size)/target_size < 0.5) and \\\n",
    "         chunk_bytes < CHUNK_MAX:\n",
    "            break\n",
    "\n",
    "        if np.product(chunks) == 1:\n",
    "            break  # Element size larger than CHUNK_MAX\n",
    "\n",
    "        chunks[idx%ndims] = np.ceil(chunks[idx%ndims] / 2.0)\n",
    "        idx += 1\n",
    "\n",
    "    return tuple(int(x) for x in chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing area below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 21, 6]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_shape_3D([9812, 720, 180])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307, 23, 6)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guess_chunk([9812, 720, 180], [9812, 720, 180], 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
